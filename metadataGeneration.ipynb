{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "metadata":{}
        "51020547154f4d45ba3f7353009616ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f4bb64cbebf4fa9875049ad0559f023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc4c69b2213d4dedb876de585c0ef1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".pdf,.docx,.txt,.png,.jpg,.jpeg",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_7dc122be1d1443bdbe1dbcc881f1c82b",
            "metadata": [
              {
                "name": "article_-_meditation_a_simple_fast_way_to_reduce_stress.pdf",
                "type": "application/pdf",
                "size": 81394,
                "lastModified": 1750850711473
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_91b6b7bb8f1b409ab6b3c00c5021738d"
          }
        },
        "7dc122be1d1443bdbe1dbcc881f1c82b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91b6b7bb8f1b409ab6b3c00c5021738d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbdeDmq1Lexn",
        "outputId": "5e9e56a7-7e4e-4b3a-fc0c-24906fed1a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/981.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.7-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.46.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.43.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading textstat-0.7.7-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.46.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=13a45313f7cb2ebe522dfbf4e26a517cc429908daa47a1f86027275a11448bb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: watchdog, python-docx, pytesseract, pyphen, PyPDF2, PyMuPDF, pdf2image, langdetect, pydeck, cmudict, textstat, streamlit\n",
            "Successfully installed PyMuPDF-1.26.1 PyPDF2-3.0.1 cmudict-1.0.32 langdetect-1.0.9 pdf2image-1.17.0 pydeck-0.9.1 pyphen-0.17.2 pytesseract-0.3.13 python-docx-1.2.0 streamlit-1.46.0 textstat-0.7.7 watchdog-6.0.0\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract pillow nltk langdetect textblob textstat spacy pdf2image python-docx PyMuPDF PyPDF2 streamlit\n",
        "!python -m nltk.downloader punkt stopwords\n",
        "!python -m textblob.download_corpora\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import docx\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from pdf2image import convert_from_path\n",
        "import fitz  # PyMuPDF - better alternative to PyPDF2\n",
        "from langdetect import detect, LangDetectException\n",
        "from textblob import TextBlob\n",
        "import textstat\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    SPACY_AVAILABLE = True\n",
        "except Exception:\n",
        "    SPACY_AVAILABLE = False\n",
        "\n",
        "# Configure Tesseract path (adjust this to your installation)\n",
        "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Check OCR availability\n",
        "try:\n",
        "    pytesseract.get_tesseract_version()\n",
        "    OCR_AVAILABLE = True\n",
        "except:\n",
        "    OCR_AVAILABLE = False"
      ],
      "metadata": {
        "id": "yuCYQZ2sLubT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf_pymupdf(file_path):\n",
        "    text = \"\"\n",
        "    extraction_log = []\n",
        "    try:\n",
        "        doc = fitz.open(file_path)\n",
        "        extraction_log.append(f\"Opened PDF with {len(doc)} pages using PyMuPDF\")\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            page_text = page.get_text()\n",
        "            if page_text.strip():\n",
        "                text += f\"\\n[Page {page_num + 1} Text]\\n{page_text}\"\n",
        "                extraction_log.append(f\"Extracted text from page {page_num + 1}\")\n",
        "            else:\n",
        "                extraction_log.append(f\"No text found in page {page_num + 1}\")\n",
        "        doc.close()\n",
        "    except Exception as e:\n",
        "        extraction_log.append(f\"PyMuPDF error: {str(e)}\")\n",
        "        return \"\", extraction_log\n",
        "    return text, extraction_log"
      ],
      "metadata": {
        "id": "QUj2pKi4L0px"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_text_from_pdf_pypdf2(file_path):\n",
        "    text = \"\"\n",
        "    extraction_log = []\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            extraction_log.append(f\"Opened PDF with {len(reader.pages)} pages using PyPDF2\")\n",
        "            if reader.is_encrypted:\n",
        "                try:\n",
        "                    reader.decrypt('')\n",
        "                    extraction_log.append(\"Decrypted PDF with empty password\")\n",
        "                except Exception as e:\n",
        "                    extraction_log.append(f\"Could not decrypt PDF: {str(e)}\")\n",
        "                    return \"\", extraction_log\n",
        "            for i, page in enumerate(reader.pages):\n",
        "                try:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text and page_text.strip():\n",
        "                        text += f\"\\n[Page {i+1} Text]\\n{page_text}\"\n",
        "                        extraction_log.append(f\"Extracted text from page {i+1}\")\n",
        "                    else:\n",
        "                        extraction_log.append(f\"No text found in page {i+1}\")\n",
        "                except Exception as e:\n",
        "                    extraction_log.append(f\"Page {i+1} error: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        extraction_log.append(f\"PyPDF2 error: {str(e)}\")\n",
        "        return \"\", extraction_log\n",
        "    return text, extraction_log"
      ],
      "metadata": {
        "id": "h-s2exRRL4Kx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def extract_text_from_pdf_ocr(file_path):\n",
        "    text = \"\"\n",
        "    extraction_log = []\n",
        "    if not OCR_AVAILABLE:\n",
        "        extraction_log.append(\"OCR not available - Tesseract not configured\")\n",
        "        return \"\", extraction_log\n",
        "    try:\n",
        "        images = convert_from_path(file_path, dpi=200, thread_count=2)\n",
        "        extraction_log.append(f\"Converted {len(images)} pages to images for OCR\")\n",
        "        for i, img in enumerate(images):\n",
        "            try:\n",
        "                custom_config = r'--oem 3 --psm 6'\n",
        "                page_text = pytesseract.image_to_string(img, config=custom_config)\n",
        "                if page_text.strip():\n",
        "                    text += f\"\\n[Page {i+1} OCR Text]\\n{page_text}\"\n",
        "                    extraction_log.append(f\"OCR processed page {i+1}\")\n",
        "                else:\n",
        "                    extraction_log.append(f\"No OCR text found on page {i+1}\")\n",
        "            except Exception as e:\n",
        "                extraction_log.append(f\"OCR failed for page {i+1}: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        extraction_log.append(f\"PDF to image conversion failed: {str(e)}\")\n",
        "        return \"\", extraction_log\n",
        "    return text, extraction_log"
      ],
      "metadata": {
        "id": "9DR75O00L8_g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    extraction_log = []\n",
        "    try:\n",
        "        text, log = extract_text_from_pdf_pymupdf(file_path)\n",
        "        extraction_log.extend(log)\n",
        "        if text.strip():\n",
        "            extraction_log.append(\"Successfully extracted text using PyMuPDF\")\n",
        "            return text, extraction_log\n",
        "    except Exception as e:\n",
        "        extraction_log.append(f\"PyMuPDF not available: {str(e)}\")\n",
        "    text, log = extract_text_from_pdf_pypdf2(file_path)\n",
        "    extraction_log.extend(log)\n",
        "    if text.strip():\n",
        "        extraction_log.append(\"Successfully extracted text using PyPDF2\")\n",
        "        return text, extraction_log\n",
        "    extraction_log.append(\"No text found with standard methods, trying OCR...\")\n",
        "    text, log = extract_text_from_pdf_ocr(file_path)\n",
        "    extraction_log.extend(log)\n",
        "    if text.strip():\n",
        "        extraction_log.append(\"Successfully extracted text using OCR\")\n",
        "        return text, extraction_log\n",
        "    else:\n",
        "        extraction_log.append(\"All extraction methods failed\")\n",
        "        return \"No text could be extracted from this PDF\", extraction_log"
      ],
      "metadata": {
        "id": "aadiIk5ML-H4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_docx(file_path):\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
        "        return text, [\"DOCX extracted successfully\"]\n",
        "    except Exception as e:\n",
        "        return f\"DOCX extraction error: {e}\", [f\"DOCX error: {str(e)}\"]"
      ],
      "metadata": {
        "id": "-re3YkouMCP0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_text_from_image(file_path):\n",
        "    try:\n",
        "        if not OCR_AVAILABLE:\n",
        "            return \"\", [\"OCR not available - Tesseract not configured\"]\n",
        "        img = Image.open(file_path)\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        custom_config = r'--oem 3 --psm 6'\n",
        "        text = pytesseract.image_to_string(img, config=custom_config)\n",
        "        if text.strip():\n",
        "            return text, [\"Image OCR successful\"]\n",
        "        else:\n",
        "            return \"\", [\"Image OCR completed but no text found\"]\n",
        "    except Exception as e:\n",
        "        return f\"Image extraction error: {e}\", [f\"Image error: {str(e)}\"]"
      ],
      "metadata": {
        "id": "uxSIA0BmMFwN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def extract_keywords(text, num_keywords=10):\n",
        "    try:\n",
        "        if not text or not isinstance(text, str):\n",
        "            return []\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
        "        words = word_tokenize(text)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [word for word in words if word.isalpha() and len(word) > 2 and word not in stop_words]\n",
        "        if not words:\n",
        "            return []\n",
        "        word_freq = Counter(words)\n",
        "        return [word for word, _ in word_freq.most_common(num_keywords)]\n",
        "    except Exception as e:\n",
        "        print(f\"Keyword extraction error: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "IY2i2-mGMIWR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_summary(text, num_sentences=3):\n",
        "    try:\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"No text available for summary\"\n",
        "        text = re.sub(r'\\[Page \\d+ .*?\\]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        sentences = sent_tokenize(text)\n",
        "        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "        if not sentences:\n",
        "            return \"No meaningful sentences found\"\n",
        "        if len(sentences) <= num_sentences:\n",
        "            return ' '.join(sentences)\n",
        "        summary = [sentences[0]]\n",
        "        if len(sentences) > 2:\n",
        "            summary.append(sentences[len(sentences)//2])\n",
        "        if len(sentences) > 1:\n",
        "            summary.append(sentences[-1])\n",
        "        return ' '.join(summary[:num_sentences])\n",
        "    except Exception as e:\n",
        "        print(f\"Summary extraction error: {e}\")\n",
        "        return \"Summary generation failed\""
      ],
      "metadata": {
        "id": "a8gx58W_MLBr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def classify_content(text):\n",
        "    try:\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"Unknown\"\n",
        "\n",
        "        import re\n",
        "        text_lower = text.lower()\n",
        "        categories = {\n",
        "            \"Legal Document\": ['contract', 'agreement', 'clause', 'terms', 'conditions', 'legal', 'whereas'],\n",
        "            \"Academic Document\": ['abstract', 'introduction', 'methodology', 'results', 'discussion', 'references'],\n",
        "            \"Report\": ['report', 'analysis', 'findings', 'conclusion', 'executive summary', 'methodology'],\n",
        "            \"Resume/CV\": ['resume', 'cv', 'curriculum vitae', 'experience', 'skills'],\n",
        "            \"Financial Document\": ['invoice', 'receipt', 'payment', 'amount', 'total', 'tax', 'billing'],\n",
        "        }\n",
        "\n",
        "        scores = {\n",
        "            category: sum(1 for word in keywords if re.search(r'\\b' + re.escape(word) + r'\\b', text_lower))\n",
        "            for category, keywords in categories.items()\n",
        "        }\n",
        "\n",
        "        best_match = max(scores, key=scores.get)\n",
        "        if scores[best_match] > 1:\n",
        "            return best_match\n",
        "        else:\n",
        "            return \"General Document\"\n",
        "    except Exception as e:\n",
        "        print(f\"Content classification error: {e}\")\n",
        "        return \"Unknown\""
      ],
      "metadata": {
        "id": "KPxRRDLzMOV1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        return 'Unknown'"
      ],
      "metadata": {
        "id": "5jmYtrLqMRFs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(text):\n",
        "    try:\n",
        "        blob = TextBlob(text)\n",
        "        polarity = blob.sentiment.polarity\n",
        "        if polarity > 0.1:\n",
        "            return 'Positive'\n",
        "        elif polarity < -0.1:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "    except Exception:\n",
        "        return 'Unknown'"
      ],
      "metadata": {
        "id": "FKq3z-naMTo2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_key_information(text):\n",
        "    \"\"\"Extract unique entities (PERSON, ORG, PLACES) with improved filtering for academic documents.\"\"\"\n",
        "    import string\n",
        "    entities = {\n",
        "        'PERSON': [],\n",
        "        'ORG': [],\n",
        "        'PLACES': []\n",
        "    }\n",
        "    # Custom ignore lists and stopwords for academic/section headers\n",
        "    ignore_words = set([\n",
        "        'Types', 'Meditation', 'Prayer', 'al', 'al.', 'the Jesus Prayer', 'Walk', 'Page', 'Text', 'the', 'and', 'or', 'of', 'in', 'on', 'for', 'with', 'to', 'by', 'at', 'from', 'as', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'it', 'this', 'that', 'these', 'those', 'a', 'an', 'but', 'if', 'because', 'while', 'where', 'when', 'which', 'who', 'whom', 'whose', 'how', 'what', 'why', 'can', 'may', 'might', 'should', 'would', 'could', 'will', 'shall', 'do', 'does', 'did', 'done', 'has', 'have', 'had', 'having', 'not', 'no', 'yes', 'etc', 'et', 'al', 'al.'\n",
        "    ])\n",
        "    min_len = 2\n",
        "    max_len = 5  # max words in an entity\n",
        "    def is_valid_entity(ent):\n",
        "        # Remove if entity is in ignore list, is all lowercase, or is a single common word\n",
        "        ent_clean = ent.strip(string.punctuation + ' ')\n",
        "        if not ent_clean:\n",
        "            return False\n",
        "        if ent_clean in ignore_words:\n",
        "            return False\n",
        "        if ent_clean.lower() in ignore_words:\n",
        "            return False\n",
        "        if ent_clean.islower() or ent_clean.isupper():\n",
        "            return False\n",
        "        if any(char.isdigit() for char in ent_clean):\n",
        "            return False\n",
        "        if len(ent_clean.split()) > max_len or len(ent_clean.split()) < min_len:\n",
        "            return False\n",
        "        if ent_clean.lower() in set([w.lower() for w in ignore_words]):\n",
        "            return False\n",
        "        return True\n",
        "    try:\n",
        "        if 'nlp' in globals() and nlp is not None:\n",
        "            doc = nlp(text)\n",
        "            persons = set()\n",
        "            orgs = set()\n",
        "            places = set()\n",
        "            for ent in doc.ents:\n",
        "                ent_text = ent.text.strip()\n",
        "                if ent.label_ == 'PERSON' and is_valid_entity(ent_text):\n",
        "                    persons.add(ent_text)\n",
        "                elif ent.label_ == 'ORG' and is_valid_entity(ent_text):\n",
        "                    orgs.add(ent_text)\n",
        "                elif ent.label_ == 'GPE' and is_valid_entity(ent_text):\n",
        "                    places.add(ent_text)\n",
        "            entities['PERSON'] = sorted(persons)\n",
        "            entities['ORG'] = sorted(orgs)\n",
        "            entities['PLACES'] = sorted(places)\n",
        "        else:\n",
        "            import re\n",
        "            names = set(n.strip() for n in re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+\\b', text) if is_valid_entity(n))\n",
        "            orgs = set(o.strip() for o in re.findall(r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\s+(Inc|Ltd|Corporation|Corp|LLC)\\b', text) if is_valid_entity(o))\n",
        "            places = set(p.strip() for p in re.findall(r'\\b([A-Z][a-z]{2,}(?:\\s+[A-Z][a-z]{2,})*)\\b', text) if is_valid_entity(p))\n",
        "            entities['PERSON'] = sorted(names)\n",
        "            entities['ORG'] = sorted(orgs)\n",
        "            entities['PLACES'] = sorted(places)\n",
        "    except Exception as e:\n",
        "        print(f\"Entity extraction failed: {e}\")\n",
        "    return entities"
      ],
      "metadata": {
        "id": "ace9v6o2MWFD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def improved_classify_content(text):\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"Unknown\"\n",
        "\n",
        "    categories = {\n",
        "        'Finance': ['invoice', 'payment', 'tax','total', 'billing', 'account', 'balance', 'bank'],\n",
        "        'Education': ['university', 'school', 'student', 'teacher', 'course', 'curriculum', 'exam', 'degree', 'academic'],\n",
        "        'Legal': ['contract', 'agreement', 'clause', 'terms', 'conditions', 'legal', 'whereas', 'law', 'court'],\n",
        "        'Medical': ['patient', 'doctor', 'medicine', 'treatment', 'diagnosis', 'hospital', 'clinical'],\n",
        "        'Technology': ['software', 'hardware', 'computer', 'technology', 'system', 'application', 'device'],\n",
        "        'Report': ['research', 'report', 'analysis', 'findings', 'conclusion', 'summary', 'methodology'],\n",
        "        'Resume/CV': ['resume', 'cv', 'curriculum vitae', 'experience', 'skills'],\n",
        "        'Academic': ['abstract', 'introduction', 'methodology', 'results', 'discussion', 'references'],\n",
        "    }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    scores = {}\n",
        "\n",
        "    for category, keywords in categories.items():\n",
        "        scores[category] = sum(1 for word in keywords if word in text_lower)\n",
        "\n",
        "    best_match = max(scores, key=scores.get)\n",
        "    if scores[best_match] > 2:\n",
        "        return best_match\n",
        "    else:\n",
        "        return 'General'"
      ],
      "metadata": {
        "id": "W8SmiuETMYC1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def generate_metadata(file_path, original_filename=None):\n",
        "    if not os.path.exists(file_path):\n",
        "        return {\"error\": \"File not found\"}\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    text = \"\"\n",
        "    extraction_log = []\n",
        "    page_count = None\n",
        "    if ext == '.pdf':\n",
        "        text, log = extract_text_from_pdf(file_path)\n",
        "        extraction_log.extend(log)\n",
        "        try:\n",
        "            doc = fitz.open(file_path)\n",
        "            page_count = len(doc)\n",
        "            doc.close()\n",
        "        except Exception:\n",
        "            page_count = None\n",
        "    elif ext == '.docx':\n",
        "        text, log = extract_text_from_docx(file_path)\n",
        "        extraction_log.extend(log)\n",
        "    elif ext in ('.png', '.jpg', '.jpeg'):\n",
        "        text, log = extract_text_from_image(file_path)\n",
        "        extraction_log.extend(log)\n",
        "    elif ext == '.txt':\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "            extraction_log.append(\"TXT file read successfully\")\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                    text = file.read()\n",
        "                extraction_log.append(\"TXT file read with latin-1 encoding\")\n",
        "            except Exception as e:\n",
        "                text = \"\"\n",
        "                extraction_log.append(f\"TXT read error: {str(e)}\")\n",
        "        except Exception as e:\n",
        "            text = \"\"\n",
        "            extraction_log.append(f\"TXT read error: {str(e)}\")\n",
        "    else:\n",
        "        extraction_log.append(f\"Unsupported file type: {ext}\")\n",
        "    try:\n",
        "        file_stats = os.stat(file_path)\n",
        "        readability_score = None\n",
        "        if text and isinstance(text, str) and len(text.split()) > 0:\n",
        "            try:\n",
        "                readability_score = textstat.flesch_reading_ease(text)\n",
        "                if readability_score is not None:\n",
        "                    readability_score = round(readability_score, 2)\n",
        "            except Exception:\n",
        "                readability_score = None\n",
        "        basic_info = {\n",
        "            \"filename\": original_filename or os.path.basename(file_path),\n",
        "            \"file_type\": ext[1:].upper() if ext else 'Unknown',\n",
        "            \"file_size\": f\"{file_stats.st_size / 1024:.2f} KB\",\n",
        "            \"processing_date\": datetime.fromtimestamp(file_stats.st_ctime).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        }\n",
        "        content_analysis = {\n",
        "            \"language\": detect_language(text) if text else 'Unknown',\n",
        "            \"word_count\": len(text.split()) if text and isinstance(text, str) else 0,\n",
        "            \"character_count\": len(text) if text and isinstance(text, str) else 0,\n",
        "            \"line_count\": text.count('\\n') + 1 if text and isinstance(text, str) else 0,\n",
        "            \"page_count\": page_count,\n",
        "            \"readability_score\": readability_score,\n",
        "            \"sentiment\": analyze_sentiment(text) if text else 'Unknown',\n",
        "            \"category\": improved_classify_content(text) if text else 'Unknown',\n",
        "            \"content type\": classify_content(text) if text else \"Unknown\",\n",
        "        }\n",
        "        entities = extract_key_information(text) if text else {}\n",
        "        semantic_data = {\n",
        "            \"summary\": extract_summary(text) if text else \"No text extracted\",\n",
        "            \"key_topics\": extract_keywords(text) if text else [],\n",
        "            \"entities\": entities,\n",
        "            \"entraction log\": extraction_log,\n",
        "            \"text preview\": text[:500] + \"...\" if text and len(text) > 500 else text or \"No text extracted\"\n",
        "        }\n",
        "        return {\n",
        "            \"basic_info\": basic_info,\n",
        "            \"content_analysis\": content_analysis,\n",
        "            \"semantic_data\": semantic_data\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'error': f\"Metadata generation failed: {str(e)}\",\n",
        "            'extraction_log': extraction_log\n",
        "        }"
      ],
      "metadata": {
        "id": "lXZBGSYmMbxU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Upload and process a file ===\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, JSON\n",
        "import tempfile\n",
        "\n",
        "# File uploader\n",
        "uploader = widgets.FileUpload(accept='.pdf,.docx,.txt,.png,.jpg,.jpeg', multiple=False)\n",
        "display(widgets.Label(\"Upload a document to extract metadata:\"))\n",
        "display(uploader)\n",
        "\n",
        "def handle_upload(change):\n",
        "    if uploader.value:\n",
        "        uploaded_file = list(uploader.value.values())[0]\n",
        "        file_name = uploaded_file['metadata']['name']\n",
        "        file_data = uploaded_file['content']\n",
        "\n",
        "        # Save to a temporary file\n",
        "        suffix = os.path.splitext(file_name)[1]\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:\n",
        "            tmp_file.write(file_data)\n",
        "            tmp_path = tmp_file.name\n",
        "\n",
        "        # Run metadata generation\n",
        "        metadata = generate_metadata(tmp_path, original_filename=file_name)\n",
        "        os.unlink(tmp_path)\n",
        "\n",
        "\n",
        "        print(\"\\n Raw JSON Output:\")\n",
        "        display(JSON(metadata, expanded=False))\n",
        "\n",
        "uploader.observe(handle_upload, names='value')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3fe024d527b54a67838e568d0fb5131b",
            "51020547154f4d45ba3f7353009616ff",
            "1f4bb64cbebf4fa9875049ad0559f023",
            "dc4c69b2213d4dedb876de585c0ef1ee",
            "7dc122be1d1443bdbe1dbcc881f1c82b",
            "91b6b7bb8f1b409ab6b3c00c5021738d"
          ]
        },
        "id": "Q1ebI98yMfql",
        "outputId": "b5210d06-4a5d-42ac-a967-6c3b23834252"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Label(value='Upload a document to extract metadata:')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fe024d527b54a67838e568d0fb5131b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.pdf,.docx,.txt,.png,.jpg,.jpeg', description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc4c69b2213d4dedb876de585c0ef1ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Raw JSON Output:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.JSON object>"
            ],
            "application/json": {
              "basic_info": {
                "filename": "article_-_meditation_a_simple_fast_way_to_reduce_stress.pdf",
                "file_type": "PDF",
                "file_size": "79.49 KB",
                "processing_date": "2025-06-25 13:07:25"
              },
              "content_analysis": {
                "language": "en",
                "word_count": 1952,
                "character_count": 12672,
                "line_count": 251,
                "page_count": 4,
                "readability_score": 48.62,
                "sentiment": "Positive",
                "category": "Medical",
                "content type": "Academic Document"
              },
              "semantic_data": {
                "summary": "Meditation: A simple, fast way to reduce stress By Mayo Clinic staff Original Article: http://www.mayoclinic.com/health/meditation/HQ01070 If stress has you anxious, tense and worried, consider trying meditation. Examples of religious mantras include the Jesus Prayer in the Christian tradition, the holy name of God in Judaism, or the om mantra of Hinduism, Buddhism and other Eastern religions. Mayo Clinic, Rochester, Minn. March 22, 2011.",
                "key_topics": [
                  "meditation",
                  "stress",
                  "health",
                  "breathing",
                  "focus",
                  "may",
                  "mind",
                  "attention",
                  "practice",
                  "body"
                ],
                "entities": {
                  "PERSON": [
                    "Horowitz S. Health",
                    "Lane JD",
                    "Lee SH",
                    "Seaward BL"
                  ],
                  "ORG": [
                    "Complementary Medicine",
                    "Complementary Therapies",
                    "Healthcare Research and Quality",
                    "International Journal of Neuroscience",
                    "Jones & Bartlett Publishers",
                    "Journal of Alternative",
                    "Journal of Psychosomatic Research",
                    "Mayo Clinic",
                    "Morning Dew Publications",
                    "the American Medical Association"
                  ],
                  "PLACES": []
                },
                "entraction log": [
                  "Opened PDF with 4 pages using PyMuPDF",
                  "Extracted text from page 1",
                  "Extracted text from page 2",
                  "Extracted text from page 3",
                  "Extracted text from page 4",
                  "Successfully extracted text using PyMuPDF"
                ],
                "text preview": "\n[Page 1 Text]\nMeditation: A simple, fast way to reduce stress \n By Mayo Clinic staff  \n \nOriginal Article:  http://www.mayoclinic.com/health/meditation/HQ01070  \n \n \nIf stress has you anxious, tense and worried, consider trying meditation. Spending even a few minutes in \nmeditation can restore your calm and inner peace.  Anyone can practice meditation. It's simple and inexpensive, \nand it doesn't require any special equipment. And you can practice meditation wherever you are — whether \nyou're o..."
              }
            }
          },
          "metadata": {
            "application/json": {
              "expanded": false,
              "root": "root"
            }
          }
        }
      ]
    }
  ]
}
